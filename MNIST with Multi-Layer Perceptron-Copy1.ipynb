{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<a href='http://www.pieriandata.com'> <img src='../Pierian_Data_Logo.png' /></a>\n",
    "___\n",
    "# MNIST Multi-Layer Perceptron\n",
    "\n",
    "In this lecture we will build out a Multi Layer Perceptron model to try to classify hand written digits using TensorFlow (a very famous example).\n",
    "\n",
    "Keep in mind that no single lecture (or course!) can cover the vastness that is Deep Learning, I would highly suggest reading MIT's [Deep Learning](http://www.deeplearningbook.org/) textbook for more information on these topics!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Data\n",
    "\n",
    "We will be using the famous MNIST data set of [handwritten digits](http://yann.lecun.com/exdb/mnist/). \n",
    "\n",
    "The images which we will be working with are black and white images of size 28 x 28 pixels, or 784 pixels total. Our features will be the pixel values for each pixel. Either the pixel is \"white\" (blank with a 0), or there is some pixel value. \n",
    "\n",
    "We will try to correctly predict what number is written down based solely on the image data in the form of an array. This type of problem (Image Recognition) is a great use case for Deep Learning Methods!\n",
    "\n",
    "This data is to Deep Learning what the iris data set is to typical machine learning algorithms.  \n",
    "\n",
    "Let's get the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Import MINST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Format\n",
    "\n",
    "The data is stored in a vector format, although the original data was a 2-dimensional matirx with values representing how much pigment was at a certain location. Let's explore this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.contrib.learn.python.learn.datasets.base.Datasets"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mnist.train.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample = mnist.train.images[3].reshape(28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x125e8e128>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAvYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x125e75cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12623c4a8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nivADSRF+ICnCDyRF+IGk/h9LcxBiIg3ABgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x125ff9160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(sample,cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "We'll need to define 4 parameters, it is really (really) hard to know what good parameter values are on a data set for which you have no experience with, however since MNIST is pretty famous, we have some reasonable values for our data below. The parameters here are:\n",
    "\n",
    "* Learning Rate - How quickly to adjust the cost function.\n",
    "* Training Epochs - How many training cycles to go through\n",
    "* Batch Size - Size of the 'batches' of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Parameters\n",
    "\n",
    "Here we have parameters which will directly define our Neural Network, these would be adjusted depending on what your data looked like and what kind of a net you would want to build. Basically just some numbers we will eventually use to define some variables later on in our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of features\n",
    "n_hidden_2 = 256 # 2nd layer number of features\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "n_samples = mnist.train.num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  TensorFlow Graph Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784]) # 784 pixels\n",
    "y = tf.placeholder(\"float\", [None, n_classes]) #9 digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiLayer Model\n",
    "\n",
    "It is time to create our model, let's review what we want to create here.\n",
    "\n",
    "First we receive the input data array and then to send it to the first hidden layer. Then the data will begin to have a weight attached to it between layers (remember this is initially a random value) and then sent to a node to undergo an activation function (along with a Bias as mentioned in the lecture). Then it will continue on to the next hidden layer, and so on until the final output layer. In our case, we will just use two hidden layers, the more you use the longer the model will take to run (but it has more of an opportunity to possibly be more accurate on the training data).\n",
    "\n",
    "Once the transformed \"data\" has reached the output layer we need to evaluate it. Here we will use a loss function (also called a cost function) to evaluate how far off we are from the desired result. In this case, how many of the classes we got correct. \n",
    "\n",
    "Then we will apply an optimization function to minimize the cost (lower the error). This is done by adjusting weight values accordingly across the network. In out example, we will use the [Adam Optimizer](http://arxiv.org/pdf/1412.6980v8.pdf), which keep in mind, relative to other mathematical concepts, is an extremely recent development.\n",
    "\n",
    "We can adjust how quickly to apply this optimization by changing our earlier learning rate parameter. The lower the rate the higher the possibility for accurate training results, but that comes at the cost of having to wait (physical time wise) for the results. Of course, after a certain point there is no benefit to lower the learning rate.\n",
    "\n",
    "Now we will create our model, we'll start with 2 hidden layers, which use the [RELU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks) activation function, which is a very simple rectifier function which essentially either returns x or zero. For our final output layer we will use a linear activation with matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multilayer_perceptron(x,weights, biases):\n",
    "    '''\n",
    "    x : Place Holder for Data Input\n",
    "    weights: Dictionary of weights\n",
    "    biases: Dicitionary of biases\n",
    "    '''\n",
    "    \n",
    "    # First Hidden layer with RELU activation\n",
    "    # X*W +B\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    # RELU (X*W+B) -> f(x) = max(0,x)\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    \n",
    "    # Second Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    \n",
    "    # Last Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights and Bias\n",
    "\n",
    "In order for our tensorflow model to work we need to create two dictionaries containing our weight and bias objects for the model. We can use the **tf.variable** object type. This is different from a constant because TensorFlow's Graph Object becomes aware of the states of all the variables. A Variable is a modifiable tensor that lives in TensorFlow's graph of interacting operations. It can be used and even modified by the computation. We will generally have the model parameters be Variables. From the documentation string:\n",
    "\n",
    "    A variable maintains state in the graph across calls to `run()`. You add a variable to the graph by constructing an instance of the class `Variable`.\n",
    "\n",
    "    The `Variable()` constructor requires an initial value for the variable, which can be a `Tensor` of any type and shape. The initial value defines the type and shape of the variable. After construction, the type and shape of the variable are fixed. The value can be changed using one of the assign methods.\n",
    "    \n",
    "We'll use tf's built-in random_normal method to create the random values for our weights and biases (you could also just pass ones as the initial biases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])), # must be row = input and col = hidden 1 for multiply\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes])) # output is same number as y\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct model\n",
    "pred = multilayer_perceptron(x, weights, biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost and Optimization Functions\n",
    "\n",
    "We'll use Tensorflow's built-in functions for this part (check out the documentation for a lot more options and discussion on this):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred,labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization of Variables\n",
    "\n",
    "Now initialize all those tf.Variable objects we created earlier. This will be the first thing we run when training our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the variables, deprecated actually changed below\n",
    "init = tf.global_variables_initializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "\n",
    "### next_batch()\n",
    "\n",
    "Before we get started I want to cover one more convenience function in our mnist data object called next_batch. This returns a tuple in the form (X,y) with an array of the data and a y array indicating the class in the form of a binary array. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = mnist.train.next_batch(10) #10 samples of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t[0]) #actual sample is 10 arrays of pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t[1]) # 10 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xsamp,ysamp = mnist.train.next_batch(1) \n",
    "# return number of samples and their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x127b86390>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noP4fMWd1xcT53+YAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x127b77320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(Xsamp.reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# Remember indexing starts at zero!\n",
    "print(ysamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Session\n",
    "Now it is time to run our session! Pay attention to how we have two loops, the outer loop which runs the epochs, and the inner loop which runs the batches for each epoch of training. Let's breakdown each step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost=199.8858\n",
      "Epoch: 2 cost=44.3322\n",
      "Epoch: 3 cost=27.5879\n",
      "Epoch: 4 cost=19.0410\n",
      "Epoch: 5 cost=13.8648\n",
      "Epoch: 6 cost=10.4134\n",
      "Epoch: 7 cost=7.7194\n",
      "Epoch: 8 cost=5.7794\n",
      "Epoch: 9 cost=4.4167\n",
      "Epoch: 10 cost=3.2017\n",
      "Epoch: 11 cost=2.4399\n",
      "Epoch: 12 cost=1.9042\n",
      "Epoch: 13 cost=1.4444\n",
      "Epoch: 14 cost=1.0658\n",
      "Epoch: 15 cost=0.8701\n",
      "Model has completed 15 Epochs of Training\n"
     ]
    }
   ],
   "source": [
    "# Launch the session, no need to initialize\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Intialize all the variables\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "# Training Epochs\n",
    "# Essentially the max amount of loops possible before we stop\n",
    "# May stop earlier if cost/loss limit was set\n",
    "for epoch in range(training_epochs):\n",
    "\n",
    "    # Start with cost = 0.0\n",
    "    avg_cost = 0.0\n",
    "\n",
    "    # Convert total number of batches to integer : total number of samples 55000 and each batch is 100 sample\n",
    "    total_batch = int(n_samples/batch_size)\n",
    "\n",
    "    # Loop over all batches in every epoch\n",
    "    for i in range(total_batch):\n",
    "\n",
    "        # Grab the next batch of training data and labels\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "\n",
    "        # Feed dictionary for optimization and loss value\n",
    "        # Returns a tuple, but we only need 'c' the cost, the first one we don't need so \n",
    "        # after we unpack we only take 'c'\n",
    "        # So we set an underscore as a \"throwaway\"\n",
    "        _, c = sess.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y})\n",
    "\n",
    "        # Compute average loss\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print(\"Epoch: {} cost={:.4f}\".format(epoch+1,avg_cost))\n",
    "\n",
    "print(\"Model has completed {} Epochs of Training\".format(training_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# OTHER WAY\n",
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "# mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    " \n",
    "# import tensorflow as tf\n",
    "# x = tf.placeholder(tf.float32, [None, 784])\n",
    " \n",
    "# W = tf.Variable(tf.zeros([784, 10]))\n",
    "# b = tf.Variable(tf.zeros([10]))\n",
    " \n",
    "# y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    " \n",
    "# y_ = tf.placeholder(tf.float32, [None, 10])\n",
    " \n",
    "# cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    " \n",
    "# train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    " \n",
    "# sess = tf.InteractiveSession()\n",
    " \n",
    "# tf.global_variables_initializer().run()\n",
    " \n",
    "# for _ in range(1000):\n",
    "#   batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "#   sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    " \n",
    "# correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    " \n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    " \n",
    "# print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluations\n",
    "\n",
    "Tensorflow comes with some built-in functions to help evaluate our model, including tf.equal and tf.cast with tf.reduce_mean.\n",
    "\n",
    "**tf.equal()**\n",
    "\n",
    "This is essentially just a check of predictions == y_test. In our case since we know the format of the labels is a 1 in an array of zeroes, we can compare argmax() location of that 1. Remember that **y** here is still that placeholder we created at the very beginning, we will perform a series of operations to get a Tensor that we can eventually fill in the test data for with an evaluation method. What we are currently running will still be empty of test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test model\n",
    "correct_predictions = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"strided_slice:0\", shape=(), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "print(correct_predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get a numerical value for our predictions we will need to use tf.cast to cast the Tensor of booleans back into a Tensor of Floating point values in order to take the mean of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_predictions = tf.cast(correct_predictions, \"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"strided_slice_1:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(correct_predictions[0]) # we need the type to be float 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the tf.reduce_mean function in order to grab the mean of the elements across the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(correct_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.ops.Tensor"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may seem a little strange, but this accuracy is still a Tensor object. Remember that we still need to pass in our actual test data! Now we can call the MNIST test labels and images and evaluate our accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.test.images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eval() method allows you to directly evaluates this tensor in a `Session` without needing to call tf.sess():mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9482\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "94% not too shabby! But this actually isn't anywhere near as good as it could be. Running for more training epochs with this data (around 20,000) can produce accuracy around 99%. But we won't do that here because that will take a very long time to run!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Great Job!\n",
    "\n",
    "### Extra Credit: See what happens if you try to make this model again with more layers!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
